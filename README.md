# Interview Lab : Power Grid Action Predictor

Welcome! This lab is designed to assess your skills in structuring a data science project, processing data, training a model, and ensuring your code is reliable through testing.

This project aims to train a <b> simple  </b> machine learning model to predict the outcome of actions on a power grid. Specifically, it predicts the maximum line overflow (rho) for each possible action, given a specific state of the power grid.

You are provided with a script, `main.py`, that simulates a power grid to create a dataset. Your mission is to build an end-to-end workflow around this script to train a model that predicts the consequences of actions on the grid.

## The Goal

Your objective is to build a small, installable Python package and a set of scripts that:

- <b>Processes the raw data generated by the simulation </b>. 
You can use and modify the code from `main.py` as a starting point. Implement a dataset caching mechanism to avoid reprocessing the same data multiple times. Create a training and testing split for the dataset.
- <b>Visualizes a key aspect of the data </b>. This can be done within a Jupyter notebook or a separate script.
- <b> Trains a machine learning model </b>. Stick to a simple model, e.g., linear regression, trees, etc.. Do not worry about performances, focus on the workflow, we want simply a proof of concept.
The model should be able to see a grid state (in the form of a feature vector) and predict the maximum line overflow (rho) for each possible action.
- <b>Includes tests </b> . You can use `pytest` or any other testing framework you prefer.
- Don't forget about version control (e.g., using `git`).

We do not provide a thorough set of instructions for each step, feel free to make decisions about the implementation details. There is no right or wrong way to approach this.

## Setup

We do not provide a complete project structure. You will need to create the necessary directories and files yourself. Feel free to use the tools and libraries you are comfortable with.

The requirements for this project are listed in the `requirements.txt` file. You can install them using `pip` and `python 3.12`:

```
pip install -r requirements.txt
```

## Details

Do not spend too much time on any one part of the project. We will be reviewing your code together during the interview. Keep stuff simple and focused on the main objectives. <b> Even if you don't finish everything, that's completely okay! </b> Simply try to think about what you would have done in that case.

## Additional resources

If you want to learn more about library we use to generate the data and interact with the power grid, check out the [Grid2op Documentation](https://grid2op.readthedocs.io/en/latest/). Do not go too deep into the details as the focus should be on the overall workflow and not how to generate the data.


------------------------------------------------------------------------------------------------

# Mon workflow

## Architecture

```bash
technical-interview/
│
├── powergrid/                     # Package principal
│   ├── __init__.py
│   ├── data.py                    # Génération et traitement des datasets
│   ├── model.py                   # Modélisation (Random Forest)
│   └── viz.py                     # Visualisation des prédictions et erreurs
│
├── scripts/                       # Scripts exécutables
│   ├── generate_data.py           # Génération et caching du dataset
│   ├── train_model.py             # Entraînement du modèle
│   └── visualize.py               # Visualisation des erreurs de prédiction
│
├── data/                          # Stockage des données
│   ├── cache/                     
│   └── cache_test/                
│
├── tests/                         # Tests unitaires
│   ├── test_cache.py 
│   ├── test_data.py                     
│   └── test_model.py 
│ 
├── results/                       # Stockage des résultats
│
├── notebook_dev.ipynb             # Notebook d'exploration
│
├── README.md
└── requirements.txt
```

## 1️⃣ Génération et traitement des données

Le script `scripts/generate_data.py` permet de :

- Créer des observations réalistes à partir de l’environnement Grid2Op (`l2rpn_case14_sandbox`).
- Extraire des features pour chaque état du réseau :  
  - `gen_p_i` / `gen_q_i` : production active et réactive des générateurs  
  - `load_p_i` / `load_q_i` : consommation active et réactive des charges  
  - `topo_i` : topologie des lignes (ouvert/fermé)  
  - `rho_i` : surcharge actuelle de chaque ligne
- Créer les targets pour chaque action (max rho sur les lignes après action).
- Implémenter un système de cache pour éviter de recalculer les données si elles existent déjà.

Exemple d’utilisation :

```bash
python scripts/generate_data.py --episodes 2 --actions 100
python scripts/generate_data.py --episodes 2 --actions 100 --force # pour forcer la régénération
```


## 2️⃣ Modélisation

On utilise simplement un Random Forest Regressor (sklearn), capable de prédire plusieurs actions simultanément.
La RMSE est calculée pour évaluer la performance du modèle. Enfin, un graphique de feature importance est généré (20 meilleures).

```bash
python scripts/train_model.py 
```

## 3️⃣ Vizualisation

On trace simplement des boxplots des erreurs de prédiction par action pour analyser la qualité des prédictions.
```bash
python scripts/viz.py 
```

## 4️⃣ Test

On inclut des tests unitaires pour garantir la robustesse du pipeline :

- Vérifier que le cache est créé et réutilisé correctement (test_cache.py).
- Vérifier que les features et targets ont la bonne forme et des colonnes cohérentes (test_data.py).
- Vérifier que le modèle peut s’entraîner et prédire sans erreurs (test_model.py)

```bash
pytest -v tests/
```
